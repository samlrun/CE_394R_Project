---
output:
  pdf_document: default
  html_document: default
---
# METHODS

```{r setup, include = FALSE}
# load R libraries here; the `include` flag in the chunk options above tells
# whether to print the results or not. Usually you don't want to print the
# library statements, or any code on the pdf.

# Main Packages ========
# I use these in every doc
library(tidyverse)
library(knitr)
library(kableExtra)
library(modelsummary)

options(dplyr.summarise.inform = FALSE)

# Other packages ------
# These sometimes get used, and sometimes don't.
library(mlogit)

# Instructions and options =========
# prints missing data in tables as blank space
options(knitr.kable.NA = '') 
# tells kableExtra to not load latex table packages in the chunk output
options(kableExtra.latex.load_packages = FALSE) 

# round and format numbers that get printed in the text of the article.
inline_hook <- function(x) {
  if (is.numeric(x)) {
    format(x, digits = 3, big.mark = ",")
  } else x
}
knitr::knit_hooks$set(inline = inline_hook)

knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
)
# options for latex-only output
if(knitr::is_latex_output()) {
  knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
} 

```

In order to determine the correlation between crash severity and other factors, 
the Pearson correlation factors were determined for different factors. The 
equation below demonstrates how to calculate the Pearson correlation factor 
using two potentially correlated variables \@ref(eq:pearson). In this experiment, 
manner of collision, presence of pedestrians, and other factors were compared to 
crash severity to determine correlation factors.

\begin{equation}
  \rho = \frac{\text{cov}(X,Y)}{\sigma_x \sigma_y}
  (\#eq:pearson)
\end{equation}


## Data

Crash data was retrieved from the Utah Department of Transportation (UDOT). The data was collected through police reports and contains information about crash severity, manner of collision, and other details. In order to perform analysis on the data, four crash files concerning crashes, crash details, location, and vehicles involved had to be combined. Then the the crashes were separated by segments and intersections to allow for consistent analysis. 

```{r load_data}
data("Car", package = "mlogit")
car_mlogit <- Car %>%
  mutate(choice = gsub("choice", "", choice)) %>%
  dfidx( varying = 5:70, shape = "wide", choice = "choice", sep = "")
```

The data was then filtered to compare the crash severities and manners of collision for segments and intersections separately.
Table \@ref(tab:datasummary) shows the Pearson correlation factors between crash severities and manners of collision for segments. The results are very similar for intersections except the correlation factors are slightly higher in general.

```{r datasummary}
# DESCRIPTIVE SUMMARY ######################################

  #df <- read_csv("data/CAMS_Crashes_14-21.csv") %>%
  #  as_tibble() %>%
  #  select(c(MANNER_COLLISION_ID,CRASH_SEVERITY_ID)) %>%
  #  filter(MANNER_COLLISION_ID != 97 & MANNER_COLLISION_ID != 99 & MANNER_COLLISION_ID != 89)
  
  #datasummary_balance(
  #  ~MANNER_COLLISION_ID, 
  #  data = df, 
  #  dinm = FALSE,
  #  title = "Descriptive Statistics of Dataset"
  #)
  
  df <- read_csv("data/CAMS_Crashes_14-21.csv") %>%
  as_tibble() %>%
  select(c(SEG_ID,MANNER_COLLISION_ID,CRASH_SEVERITY_ID)) %>%
  filter(MANNER_COLLISION_ID != 97 & MANNER_COLLISION_ID != 99 & MANNER_COLLISION_ID != 89) %>%
  group_by(SEG_ID) %>%
  summarize(
    Angle = sum(MANNER_COLLISION_ID == 1),
    Front_to_Rear = sum(MANNER_COLLISION_ID == 2),
    Head_On = sum(MANNER_COLLISION_ID == 3),
    Sideswipe_Same_Dir = sum(MANNER_COLLISION_ID == 4),
    Sideswipe_Opp_Dir = sum(MANNER_COLLISION_ID == 5),
    Parked_Veh = sum(MANNER_COLLISION_ID == 6),
    Rear_to_Side = sum(MANNER_COLLISION_ID == 7),
    Rear_to_Rear = sum(MANNER_COLLISION_ID == 8),
    Single_Veh = sum(MANNER_COLLISION_ID == 96),
    Sev_1 = sum(CRASH_SEVERITY_ID == 1),
    Sev_2 = sum(CRASH_SEVERITY_ID == 2),
    Sev_3 = sum(CRASH_SEVERITY_ID == 3),
    Sev_4 = sum(CRASH_SEVERITY_ID == 4),
    Sev_5 = sum(CRASH_SEVERITY_ID == 5)
  ) %>%
  select(-SEG_ID)

  colnames(df) <- c(
    "Angle",
    "Front to Rear",
    "Head on",
    "Sideswipe in the Same Direction",
    "Sideswipe in the Opposite Direction",
    "Collision with Parked Vehicle",
    "Rear to Side",
    "Rear to Rear",
    "Single Vehicle Crash",
    "Severity 1 (Property Damage Only)",
    "Severity 2 (Possible Injury)",
    "Severity 3 (Suspected Minor Injury)",
    "Severity 4 (Suspected Major Injury)",
    "Severity 5 (Fatal Injury)"
  )

  # Correlation matrix for data frame
  cortable <- df %>%
    cor() %>%
    round(2)
  
  cortable <- cortable[1:9,10:14] %>%
    as.data.frame()
  
  cortable %>%
    kbl(booktabs = T, caption = "Correlation Matrix", linesep = "\\addlinespace", align = "c") %>%
    column_spec(1, bold = T, width = "8em") %>%
    column_spec(2, width = "8em") %>%
    column_spec(3, width = "8em") %>%
    column_spec(4, width = "8em") %>%
    column_spec(5, width = "8em") %>%
    column_spec(6, width = "8em") %>%
    kable_styling(
      latex_options = c("scale_down","striped"),
      bootstrap_options = "condensed",
      full_width = F
      )

```

## Models

The segmentation method used in this model is based on characteristics like AADT and length to facilitate homogeneous segments. Intersection related crashes were determined by estimating the location of the stop bar given approach speeds. Considering that the segments and intersection are homogeneous and consistent, it is possible to calculate correlation between crash severities and other factors. 

(I hope to include correlation to other factors like the presence of bus stops and schools near intersections, but I have a little more to do to prepare that data.)